{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths\n",
    "OUTPUT_FOLDER = \"/network/scratch/s/sonia.joseph/sae_checkpoints/TinyCLIP40M/images\" # output directory\n",
    "import os\n",
    "os.makedirs(OUTPUT_FOLDER,exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "CHECKPOINT_PATH= \"\" # trainingscirp checkpoint path #TODO shouldn't be needed.\n",
    "IMAGENET_PATH = \"\" #'folder containing imagenet1k data organized as follows: https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description'\n",
    "\n",
    "SAE_PATH = \"/network/scratch/s/sonia.joseph/sae_checkpoints/TinyCLIP40M/checkpoints/rob_sae\"# path to SAE folder, might look something like \"final_sae_group_wkcn_TinyCLIP-ViT-40M-32-Text-19M-LAION400M_blocks.{layer}.mlp.hook_post_16384\"\n",
    "AUTOENCODER_NAME = \"\" #name of the particular sae within group (all names will get printed below )\n",
    "#model specs TODO these should be infered from pretrained model checkpoint (if they aren't already)\n",
    "LAYERS =  9\n",
    "EXPANSION_FACTOR = 8\n",
    "D_IN = 2048\n",
    "MODEL_NAME = \"wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M\"\n",
    "CONTEXT_SIZE = 50 \n",
    "PATCH_SIZE = 32\n",
    "HOOKPOINT = \"blocks.{layer}.mlp.hook_post\"\n",
    "LEGACY_LOAD= False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108668d604b74186aa5794e23c6921c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(â€¦)-LAION400M_blocks.9.hook_mlp_out_8192.pt:   0%|          | 0.00/33.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sae.language'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m model_path \u001b[38;5;241m=\u001b[39m hf_hub_download(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, filename\u001b[38;5;241m=\u001b[39mfilename, cache_dir\u001b[38;5;241m=\u001b[39mSAE_PATH)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m model:\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m i\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/env/lib/python3.10/site-packages/torch/serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sae.language'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "# Set up the repository and model name\n",
    "repo_id = 'Prisma-Multimodal/sae_weights'\n",
    "filename = 'final_sae_group_wkcn_TinyCLIP-ViT-40M-32-Text-19M-LAION400M_blocks.9.hook_mlp_out_8192.pt'\n",
    "\n",
    "# Download the model\n",
    "model_path = hf_hub_download(repo_id=repo_id, filename=filename, cache_dir=SAE_PATH)\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "for i in model:\n",
    "    model = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval constants\n",
    "\n",
    "EVAL_MAX = 50_000 \n",
    "BATCH_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 457) (main.py, line 457)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/env/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\n\u001b[0;31m    from sae.main import setup, ImageNetValidationDataset\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/ViT-Prisma/src/sae/main.py:457\u001b[0;36m\u001b[0m\n\u001b[0;31m    parser.add_argument('--checkpoint_path', default='/network/scratch/s/sonia.joseph/sae_checkpoints)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 457)\n"
     ]
    }
   ],
   "source": [
    "from sae.main import setup, ImageNetValidationDataset\n",
    "import torch\n",
    "import plotly.express as px\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "import einops\n",
    "from transformers import CLIPProcessor\n",
    "from vit_prisma.utils.data_utils.imagenet_dict import IMAGENET_DICT\n",
    "from typing import List\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup eval data \n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "data_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    #TODO for clip only \n",
    "    torchvision.transforms.Normalize(mean=clip_processor.image_processor.image_mean,\n",
    "                        std=clip_processor.image_processor.image_std), ])\n",
    "\n",
    "\n",
    "\n",
    "# assuming the same structure as here: https://www.kaggle.com/c/imagenet-object-localization-challenge/overview/description\n",
    "imagenet_val_path  =os.path.join(IMAGENET_PATH, \"ILSVRC/Data/CLS-LOC/val\")\n",
    "imagenet_val_labels = os.path.join(IMAGENET_PATH, \"LOC_val_solution.csv\")\n",
    "imagenet_label_strings = os.path.join(IMAGENET_PATH, \"LOC_synset_mapping.txt\" )\n",
    "imagenet_data = ImageNetValidationDataset(imagenet_val_path,imagenet_label_strings, imagenet_val_labels ,data_transforms, return_index=True)\n",
    "imagenet_data_visualize = ImageNetValidationDataset(imagenet_val_path,imagenet_label_strings, imagenet_val_labels ,torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    torchvision.transforms.ToTensor(),]), return_index=True)\n",
    "\n",
    "data_loader = DataLoader(imagenet_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "ind_to_name = {}\n",
    "\n",
    "with open( os.path.join(IMAGENET_PATH, \"LOC_synset_mapping.txt\" ), 'r') as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line_num, line in enumerate(file):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        parts = line.split(' ')\n",
    "        label = parts[1].split(',')[0]\n",
    "        ind_to_name[line_num] = label\n",
    "\n",
    "\n",
    "# setup model\n",
    "cfg ,model, activations_loader, sae_group = setup(checkpoint_path=CHECKPOINT_PATH, \n",
    "                                                  imagenet_path=IMAGENET_PATH ,\n",
    "                                                    pretrained_path=SAE_PATH, layers= LAYERS, expansion_factor=EXPANSION_FACTOR,\n",
    "                                                    model_name=MODEL_NAME, context_size=CONTEXT_SIZE, d_in=D_IN, hook_point=HOOKPOINT, legacy_load=LEGACY_LOAD)\n",
    "model = model.to(device)\n",
    "for i, (name, sae) in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Name: {name} Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )\n",
    "\n",
    "sparse_autoencoder = sae_group.autoencoders[AUTOENCODER_NAME]\n",
    "sparse_autoencoder = sparse_autoencoder.to(device)\n",
    "layer_num = sparse_autoencoder.cfg.hook_point_layer\n",
    "print(f\"Chosen layer {layer_num} hook point {sparse_autoencoder.cfg.hook_point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L0 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with torch.no_grad():\n",
    "    batch_tokens, labels = activations_loader.get_val_batch_tokens()\n",
    "    _, cache = model.run_with_cache(batch_tokens)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARENA stuff\n",
    "https://arena3-chapter1-transformer-interp.streamlit.app/[1.4]_Superposition_&_SAEs\n",
    "first getting feature probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper functions\n",
    "update_layout_set = {\"xaxis_range\", \"yaxis_range\", \"hovermode\", \"xaxis_title\", \"yaxis_title\", \"colorbar\", \"colorscale\", \"coloraxis\", \"title_x\", \"bargap\", \"bargroupgap\", \"xaxis_tickformat\", \"yaxis_tickformat\", \"title_y\", \"legend_title_text\", \"xaxis_showgrid\", \"xaxis_gridwidth\", \"xaxis_gridcolor\", \"yaxis_showgrid\", \"yaxis_gridwidth\", \"yaxis_gridcolor\", \"showlegend\", \"xaxis_tickmode\", \"yaxis_tickmode\", \"margin\", \"xaxis_visible\", \"yaxis_visible\", \"bargap\", \"bargroupgap\", \"coloraxis_showscale\"}\n",
    "def to_numpy(tensor):\n",
    "    \"\"\"\n",
    "    Helper function to convert a tensor to a numpy array. Also works on lists, tuples, and numpy arrays.\n",
    "    \"\"\"\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return tensor\n",
    "    elif isinstance(tensor, (list, tuple)):\n",
    "        array = np.array(tensor)\n",
    "        return array\n",
    "    elif isinstance(tensor, (torch.Tensor, torch.nn.parameter.Parameter)):\n",
    "        return tensor.detach().cpu().numpy()\n",
    "    elif isinstance(tensor, (int, float, bool, str)):\n",
    "        return np.array(tensor)\n",
    "    else:\n",
    "        raise ValueError(f\"Input to to_numpy has invalid type: {type(tensor)}\")\n",
    "\n",
    "def hist(tensor, save_name, show=True, renderer=None, **kwargs):\n",
    "    '''\n",
    "    '''\n",
    "    kwargs_post = {k: v for k, v in kwargs.items() if k in update_layout_set}\n",
    "    kwargs_pre = {k: v for k, v in kwargs.items() if k not in update_layout_set}\n",
    "    if \"bargap\" not in kwargs_post:\n",
    "        kwargs_post[\"bargap\"] = 0.1\n",
    "    if \"margin\" in kwargs_post and isinstance(kwargs_post[\"margin\"], int):\n",
    "        kwargs_post[\"margin\"] = dict.fromkeys(list(\"tblr\"), kwargs_post[\"margin\"])\n",
    "\n",
    "    histogram_fig = px.histogram(x=to_numpy(tensor), **kwargs_pre)\n",
    "    histogram_fig.update_layout(**kwargs_post)\n",
    "\n",
    "    # Save the figure as a PNG file\n",
    "    histogram_fig.write_image(os.path.join(OUTPUT_FOLDER, f\"{save_name}.png\"))\n",
    "    if show:\n",
    "        px.histogram(x=to_numpy(tensor), **kwargs_pre).update_layout(**kwargs_post).show(renderer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_feature_probability(\n",
    "    images,\n",
    "    model,\n",
    "    sparse_autoencoder,\n",
    "):\n",
    "    '''\n",
    "    Returns the feature probabilities (i.e. fraction of time the feature is active) for each feature in the\n",
    "    autoencoder, averaged over all `batch * seq` tokens.\n",
    "    '''\n",
    "    _, cache = model.run_with_cache(images)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    class_acts = feature_acts[:, 0, :]\n",
    "    post_reshaped = einops.repeat(feature_acts, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "\n",
    "    return post_reshaped.mean(0), class_acts.mean(0)\n",
    "\n",
    "total_acts = None\n",
    "total_class_acts = None\n",
    "this_max = EVAL_MAX\n",
    "for batch_idx, (total_images, total_labels, total_indices) in tqdm(enumerate(data_loader), total=this_max//BATCH_SIZE):\n",
    "        total_images = total_images.to(device)\n",
    "        new, new_class = get_feature_probability(total_images, model, sparse_autoencoder)\n",
    "\n",
    "        if total_acts is None:\n",
    "             total_acts = new\n",
    "             total_class_acts = new_class \n",
    "        else:\n",
    "             total_acts = total_acts + new \n",
    "             total_class_acts = total_class_acts + new_class\n",
    "\n",
    "\n",
    "        if batch_idx*BATCH_SIZE >= this_max:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_probability = total_acts/(this_max//BATCH_SIZE)\n",
    "\n",
    "log_freq = (feature_probability + 1e-10).log10()\n",
    "\n",
    "feature_probability_class = total_class_acts/(this_max//BATCH_SIZE)\n",
    "\n",
    "log_freq_class = (feature_probability_class + 1e-10).log10()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_probability)\n",
    "def visualize_sparsities(log_freq, conditions, condition_texts, name):\n",
    "    # Visualise sparsities for each instance\n",
    "    hist(\n",
    "        log_freq,\n",
    "        f\"{name}_frequency_histogram\",\n",
    "        show=True,\n",
    "        title=f\"{name} Log Frequency of Features\",\n",
    "        labels={\"x\": \"log<sub>10</sub>(freq)\"},\n",
    "        histnorm=\"percent\",\n",
    "        template=\"ggplot2\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    for condition, condition_text in zip(conditions, condition_texts):\n",
    "        percentage = (torch.count_nonzero(condition)/log_freq.shape[0]).item()*100\n",
    "        if percentage == 0:\n",
    "            continue\n",
    "        percentage = int(np.round(percentage))\n",
    "        rare_encoder_directions = sparse_autoencoder.W_enc[:, condition]\n",
    "        rare_encoder_directions_normalized = rare_encoder_directions / rare_encoder_directions.norm(dim=0, keepdim=True)\n",
    "\n",
    "        # Compute their pairwise cosine similarities & sample randomly from this N*N matrix of similarities\n",
    "        cos_sims_rare = (rare_encoder_directions_normalized.T @ rare_encoder_directions_normalized).flatten()\n",
    "        cos_sims_rare_random_sample = cos_sims_rare[torch.randint(0, cos_sims_rare.shape[0], (10000,))]\n",
    "\n",
    "        # Plot results\n",
    "        hist(\n",
    "            cos_sims_rare_random_sample,\n",
    "            f\"{name}_low_prop_similarity_{condition_text}\",\n",
    "            show=True,\n",
    "            marginal=\"box\",\n",
    "            title=f\"{name} Cosine similarities of random {condition_text} encoder directions with each other ({percentage}% of features)\",\n",
    "            labels={\"x\": \"Cosine sim\"},\n",
    "            histnorm=\"percent\",\n",
    "            template=\"ggplot2\",\n",
    "        )\n",
    "\n",
    "#TODO these conditions should be tuned to distribution of your data!\n",
    "conditions = [ torch.logical_and(log_freq < -4,log_freq > -5),torch.logical_and(log_freq > -4,log_freq < -2),log_freq>-2, log_freq <-8, torch.logical_and(log_freq < -4,log_freq > -6.5),torch.logical_and(log_freq < -6.5,log_freq > -8)]\n",
    "condition_texts = [  \"logfreq_[-5,-4]\", \"logfreq_[-4,-2]\", \"logfreq_[-2,inf]\",\"logfreq_[-inf,-8]\", \"logfreq_[-6.5,-4]\", \"logfreq_[-8,-6.5]\",]\n",
    "visualize_sparsities(log_freq, conditions, condition_texts, \"TOTAL\")\n",
    "conditions_class = [torch.logical_and(log_freq_class < -4,log_freq_class > -8), log_freq_class <-9, log_freq_class>-4]\n",
    "condition_texts_class = [\"logfreq_[-8,-4]\", \"logfreq_[-inf,-9]\",\"logfreq_[-4,inf]\"]\n",
    "visualize_sparsities(log_freq_class, conditions_class, condition_texts_class,\"CLS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reconstruction and substitution loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_reconstruction_loss(\n",
    "    images,\n",
    "    model,\n",
    "    autoencoder,\n",
    "):\n",
    "    '''\n",
    "    Returns the reconstruction loss of each autoencoder instance on the given batch of tokens (i.e.\n",
    "    the L2 loss between the activations and the autoencoder's reconstructions, averaged over all tokens).\n",
    "    '''\n",
    "\n",
    "    logits, cache = model.run_with_cache(images)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, mse_loss_ghost_resid = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "\n",
    "    # Print out the avg L2 norm of activations\n",
    "    print(\"Avg L2 norm of acts: \", cache[sparse_autoencoder.cfg.hook_point].pow(2).mean().item())\n",
    "\n",
    "    # Print out the cosine similarity between original neuron activations & reconstructions (averaged over neurons)\n",
    "    print(\"Avg cos sim of neuron reconstructions: \", torch.cosine_similarity(einops.rearrange( cache[sparse_autoencoder.cfg.hook_point], \"batch seq d_mlp -> (batch seq) d_mlp\"),\n",
    "                                                                              einops.rearrange( sae_out, \"batch seq d_mlp -> (batch seq) d_mlp\"),\n",
    "                                                                                dim=0).mean(-1).tolist())\n",
    "    print(\"l1\", l1_loss.sum().item())\n",
    "    return mse_loss.item()\n",
    "\n",
    "this_max = 4\n",
    "count = 0\n",
    "print(sparse_autoencoder.cfg.hook_point)\n",
    "for batch_idx, (total_images, total_labels, total_indices) in enumerate(data_loader):\n",
    "        total_images = total_images.to(device)\n",
    "        reconstruction_loss = get_reconstruction_loss(total_images, model, sparse_autoencoder)\n",
    "        print(\"mse\", reconstruction_loss)\n",
    "\n",
    "\n",
    "\n",
    "        if batch_idx >= this_max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "Language model results for comparsion\n",
    "Avg L2 norm of acts:  0.11062075197696686\n",
    "Avg cos sim of neuron reconstructions:  0.8348199129104614\n",
    "l1 19.51767921447754\n",
    "mse 0.043452925980091095\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get random features from different bins\n",
    "\n",
    "interesting_features_indices = []\n",
    "interesting_features_values = []\n",
    "interesting_features_category = []\n",
    "number_features_per = 50\n",
    "for condition, condition_text in zip(conditions + conditions_class, [f\"TOTAL_{c}\" for c in condition_texts] + [f\"CLS_{c}\" for c in condition_texts_class]):\n",
    "    \n",
    "\n",
    "    potential_indices = torch.nonzero(condition, as_tuple=True)[0]\n",
    "\n",
    "    # Shuffle these indices and select a subset\n",
    "    sampled_indices = potential_indices[torch.randperm(len(potential_indices))[:number_features_per]]\n",
    "\n",
    "    values = log_freq[sampled_indices]\n",
    "\n",
    "    interesting_features_indices = interesting_features_indices + sampled_indices.tolist()\n",
    "    interesting_features_values = interesting_features_values + values.tolist()\n",
    "\n",
    "    interesting_features_category = interesting_features_category + [f\"{condition_text}\"]*len(sampled_indices)\n",
    "\n",
    "\n",
    "# for v,i, c in zip(interesting_features_indices, interesting_features_values, interesting_features_category):\n",
    "#     print(c, v,i)\n",
    "\n",
    "print(set(interesting_features_category))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.no_grad()\n",
    "def highest_activating_tokens(\n",
    "    images,\n",
    "    model,\n",
    "    sparse_autoencoder,\n",
    "    W_enc,\n",
    "    b_enc,\n",
    "    feature_ids: List[int],\n",
    "    feature_categories,\n",
    "    k: int = 10,\n",
    "):\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "\n",
    "    # Get the post activations from the clean run\n",
    "    _, cache = model.run_with_cache(images)\n",
    "\n",
    "    inp = cache[sparse_autoencoder.cfg.hook_point]\n",
    "    b, seq_len, _ = inp.shape\n",
    "    post_reshaped = einops.rearrange( inp, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            W_enc,\n",
    "            \"... d_in, d_in n -> ... n\",\n",
    "        )\n",
    "    \n",
    "    acts = acts + b_enc\n",
    "    acts = torch.nn.functional.relu(acts)\n",
    "    #TODO clean up\n",
    "    unshape = einops.rearrange(acts, \"(batch seq) d_in -> batch seq d_in\", batch=b, seq=seq_len)\n",
    "    cls_acts = unshape[:,0,:]\n",
    "    per_image_acts = unshape.mean(1)\n",
    "\n",
    "\n",
    "\n",
    "    to_return = {} \n",
    "    #TODO this is a bad way to do it.\n",
    "    for i, (feature_id, feature_cat) in enumerate(zip(feature_ids, feature_categories)):\n",
    "        if \"CLS_\" in feature_cat:\n",
    "            top_acts_values, top_acts_indices = cls_acts[:,i].topk(k)\n",
    "\n",
    "            to_return[feature_id]  = (top_acts_indices, top_acts_values)\n",
    "        else:\n",
    "            top_acts_values, top_acts_indices = per_image_acts[:,i].topk(k)\n",
    "\n",
    "            to_return[feature_id]  = (top_acts_indices, top_acts_values)\n",
    "    return to_return \n",
    "this_max = EVAL_MAX\n",
    "\n",
    "max_indices = {i:None for i in interesting_features_indices}\n",
    "max_values =  {i:None for i in interesting_features_indices} \n",
    "b_enc = sparse_autoencoder.b_enc[interesting_features_indices]\n",
    "W_enc = sparse_autoencoder.W_enc[:, interesting_features_indices]\n",
    "for batch_idx, (total_images, total_labels, total_indices) in tqdm(enumerate(data_loader), total=this_max//BATCH_SIZE):\n",
    "        total_images = total_images.to(device)\n",
    "        total_indices = total_indices.to(device)\n",
    "        new_stuff = highest_activating_tokens(total_images, model, sparse_autoencoder, W_enc, b_enc, interesting_features_indices, interesting_features_category, k=16)\n",
    "        for feature_id in interesting_features_indices:\n",
    "\n",
    "            new_indices, new_values = new_stuff[feature_id]\n",
    "            new_indices = total_indices[new_indices]\n",
    "            #  new_indices[:,0] = new_indices[:,0] + batch_idx*batch_size\n",
    "            \n",
    "            if max_indices[feature_id] is None:\n",
    "                max_indices[feature_id] = new_indices\n",
    "                max_values[feature_id] = new_values\n",
    "            else:\n",
    "                ABvals = torch.cat((max_values[feature_id], new_values))\n",
    "                ABinds = torch.cat((max_indices[feature_id], new_indices))\n",
    "                _, inds = torch.topk(ABvals, new_values.shape[0])\n",
    "                max_values[feature_id] = ABvals[inds]\n",
    "                max_indices[feature_id] = ABinds[inds]\n",
    "    \n",
    "\n",
    "        if batch_idx*BATCH_SIZE >= this_max:\n",
    "            break\n",
    "top_per_feature = {i:(max_values[i].detach().cpu(), max_indices[i].detach().cpu()) for i in interesting_features_indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.no_grad()\n",
    "def get_heatmap(\n",
    "          \n",
    "          image,\n",
    "          model,\n",
    "          sparse_autoencoder,\n",
    "          feature_id,\n",
    "): \n",
    "    image = image.to(device)\n",
    "    _, cache = model.run_with_cache(image.unsqueeze(0))\n",
    "\n",
    "    post_reshaped = einops.rearrange( cache[sparse_autoencoder.cfg.hook_point], \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            sparse_autoencoder.W_enc[:, feature_id],\n",
    "            \"x d_in, d_in -> x\",\n",
    "        )\n",
    "    return acts \n",
    "     \n",
    "def image_patch_heatmap(activation_values,image_size=224, pixel_num=14):\n",
    "    activation_values = activation_values.detach().cpu().numpy()\n",
    "    activation_values = activation_values[1:]\n",
    "    activation_values = activation_values.reshape(pixel_num, pixel_num)\n",
    "\n",
    "    # Create a heatmap overlay\n",
    "    heatmap = np.zeros((image_size, image_size))\n",
    "    patch_size = image_size // pixel_num\n",
    "\n",
    "    for i in range(pixel_num):\n",
    "        for j in range(pixel_num):\n",
    "            heatmap[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size] = activation_values[i, j]\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "    # Removing axes\n",
    "\n",
    "\n",
    "for feature_ids, cat, logfreq in tqdm(zip(top_per_feature.keys(), interesting_features_category, interesting_features_values), total=len(interesting_features_category)):\n",
    "  #  print(f\"looking at {feature_ids}, {cat}\")\n",
    "    max_vals, max_inds = top_per_feature[feature_ids]\n",
    "    images = []\n",
    "    model_images = []\n",
    "    gt_labels = []\n",
    "    for bid, v in zip(max_inds, max_vals):\n",
    "\n",
    "        image, label, image_ind = imagenet_data_visualize[bid]\n",
    "\n",
    "        assert image_ind.item() == bid\n",
    "        images.append(image)\n",
    "\n",
    "        model_img, _, _ = imagenet_data[bid]\n",
    "        model_images.append(model_img)\n",
    "        gt_labels.append(ind_to_name[label])\n",
    "    \n",
    "    grid_size = int(np.ceil(np.sqrt(len(images))))\n",
    "    fig, axs = plt.subplots(int(np.ceil(len(images)/grid_size)), grid_size, figsize=(15, 15))\n",
    "    name=  f\"Category: {cat},  Feature: {feature_ids}\"\n",
    "    fig.suptitle(name)#, y=0.95)\n",
    "    for ax in axs.flatten():\n",
    "        ax.axis('off')\n",
    "    complete_bid = []\n",
    "\n",
    "    for i, (image_tensor, label, val, bid,model_img) in enumerate(zip(images, gt_labels, max_vals,max_inds,model_images )):\n",
    "        if bid in complete_bid:\n",
    "            continue \n",
    "        complete_bid.append(bid)\n",
    "\n",
    "\n",
    "\n",
    "        row = i // grid_size\n",
    "        col = i % grid_size\n",
    "        heatmap = get_heatmap(model_img,model,sparse_autoencoder, feature_ids )\n",
    "        heatmap = image_patch_heatmap(heatmap, pixel_num=224//PATCH_SIZE)\n",
    "\n",
    "        display = image_tensor.numpy().transpose(1, 2, 0)\n",
    "\n",
    "        has_zero = False\n",
    "        \n",
    "\n",
    "        axs[row, col].imshow(display)\n",
    "        axs[row, col].imshow(heatmap, cmap='viridis', alpha=0.3)  # Overlaying the heatmap\n",
    "        axs[row, col].set_title(f\"{label} {val.item():0.03f} {'class token!' if has_zero else ''}\")  \n",
    "        axs[row, col].axis('off')  \n",
    "\n",
    "    plt.tight_layout()\n",
    "    folder = os.path.join(OUTPUT_FOLDER, f\"{cat}\")\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    plt.savefig(os.path.join(folder, f\"neglogfreq_{-logfreq}feauture_id_{feature_ids}.png\"))\n",
    "    plt.close()\n",
    "   # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
